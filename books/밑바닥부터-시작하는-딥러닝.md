# 밑바닥부터 시작하는 딥러닝



## 01. 헬로 파이썬

### numpy

```
# broadcasting
A = np.array([[1, 2], [3, 4]])
B = np.array([10, 20])

A * B

[ins] In [9]: X
Out[9]: array([51, 55, 14, 19,  0,  4])

[ins] In [10]: X > 15
Out[10]: array([ True,  True, False,  True, False, False])

[ins] In [11]: X[X>15]
Out[11]: array([51, 55, 19])
```


## 02. 퍼셉트론

### 퍼셉트론이란?

perceptron 알고리즘: 다수의 신호를 입력으로 받아 하나의 신호를 출력

- 입력 신호가 뉴런(노드)에 보내질 때 각각 가중치가 곱해짐
- 이 총 합이 정해진 한계(임계값)를 넘을때에만 1을 출력
- 아닐 경우 0을 출력

```
y = 0 if (w1 * x1 + w2 * x2) >= point
y = 1 if (w1 * x1 + w2 * x2) < point
```

이를 만족하는 w1, w2, theta (임계값) 조합은 여러개가 있을 수 있음



```
(w1, w2, theta)
# AND의 경우
(1, 1, 1)
(0.5, 0.5, 0.7)
...

# OR의 경우
(1, 1, 0)
(0.5, 0.5, 1)
...
```


### 파이썬으로 퍼셉트론 구현

```
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```

### 가중치와 편향 도입

위의 식을 다른 방식으로 표현하면 아래와 같다. 여기에서 `b`를 편향 (bias)이라고 함

```
y = 0 if (b + w1*x1 + w2*x2) <= 0
y = 1 if (b + w1*x1 + w2*x2) > 0
```

이를 numpy array로 표현해보면 다음과 같음

```
def AND(x1, x2):
    w1, w2 = 0.5, 0.5
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
        
def NAND(x1, x2):
    w1, w2 = 0.5, 0.5
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
        
def OR(x1, x2):
    w1, w2 = 0.5, 0.5
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

기존에 theta가 편향 b로 치환됨. (theta -> -b)

위 함수에서 가중치 (w, b)만 변경하면 AND 대신 OR, NAND도 구현 가능하다.

### 퍼셉트론의 한계

위에서 AND, OR, NAND를 구현해 보았는데 이번에는 XOR 게이트를 구현해보자.

> XOR 게이트: 배타적 논리합. 한 쪽이 1일때에만 1을 출력.

> 파이썬에서는 `^` operator가 XOR을 의미한다.

XOR는 퍼셉트론으로 표현하지 못하는 것은 그래프를 그리면 쉽게 알 수 있다.

(0, 0), (1, 1), (0, 1), (1, 0) 네 점 중에서 0, 1을 출력하는 점들을 직선으로 나눌 수 있다면 퍼셉트론으로 구현이 가능하다.

반면 XOR의 진리표는 다음과 같다.

```
| x1 | x2 | y |
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |
```

이를 x1, x2 축을 가진 그래프에 찍어보면 직선으로 0, 1을 나누는 영역을 그릴 수 없다.

### 다중 퍼셉트론

퍼셉트론을 층을 쌓아 multi-layer perceptron을 만들 수 있다.

XOR를 다중 퍼셉트론으로 구현해보자.

일단 제일 쉬운 방법은 OR, AND, NAND를 조합하여 만드는 방법.

```
# AND
| x1 | x2 | y |
| 0  | 0  | 0 |
| 1  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 1  | 1 |

# OR
| x1 | x2 | y |
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 1 |

# NAND
| x1 | x2 | y |
| 0  | 0  | 1 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |
```

OR, NAND의 게이트를 적용시키고 그 결과를 다시 AND에 입력하면 XOR를 구현할 수 있다.

![img](https://i.imgur.com/6hoX7A6.png)

이를 파이썬으로 구현해보면 다음과 같다.

```
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```


이 퍼셉트론으로 간단한 함수부터 컵퓨터 같은 복잡한 함수도 표현 가능하다.

## 02. 신경망

퍼셉트론으로 복잡한 함수도 표현이 가능하지만, 가중치는 여전히 사람이 수동으로 설정해야 함.

신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습 가능.

### 퍼셉트론 복습

기존 퍼셉트론의 함수를 `h(x)`를 사용하여 감싸보자

```
# 기존 함수
y = 0 if (b + w1*x1 + w2*x2) <= 0
y = 1 if (b + w1*x1 + w2*x2) > 0

# 활성화 함수
def h(x):
    if x <= 0:
        return 0
    elif x > 0:
        return 1
        
y = h(b + w1*x1 + w2*x2)
```

위와 같이 입력 신호의 총 합을 출력 신호로 변환하는 함수를 활성화 함수 (activation function)이라고 함.

### 활성화 함수

위의 활성화 함수가 퍼셉트론에서 신경망으로 가기 위한 길잡이.

퍼셉트론에서는 활성화 함수로 계단 함수를 이용.

> 계단 함수(step function): 임계값을 경계로 출력이 바뀌는 함수

계단 함수 이외의 함수를 사용한다면? 실제로 신경망에서는 다른 함수를 활성화 함수로 사용함.

다음부터 신경망에서 사용하는 활성화 함수 소개.

### 시그모이드 함수

![img](https://i.imgur.com/ZhXXone.png)

이를 함수로 표현해보면

```
h(x) = 1/(1 + exp(-x)
```

> `e`는 자연상수로 `2.7182...`의 값을 갖는 실수

신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환.

퍼셉트론과 신경망의 주된 차이는 활성화 함수뿐 뉴런이 여러 층으로 이어지는 구조와 신호를 전달하는 방법은 기본적으로 퍼셉트론과 같다.

### 계단 함수 구현하기

함수 이해를 위해 직접 파이썬으로 먼저 계단 함수를 구현

```
def step_function(x1 -> int, x2 -> int) -> int:
    if x > 0:
        return 1
    else:
        return 0
```

가장 간단한 구현이지만 x로 실수만 받을 수 있다.

넘파이 배열을 인수로 넣기 위해서 다음과 같이 변경.

```
from typing import Callable

def step_function(x: Callable[[np.ndarray], np.ndarray]) -> int:
    y = x > 0
    return y.astype(np.int)
```

numpy 배열에 부등호 연산을 수행하면 bool 배열을 반환하고, `astype`을 사용하여 int형으로 변환해준다.

위 함수의 그래프를 그려보자.

```
from typing import Callable
import numpy as np
import matplotlib.pylab as plt

def step_function(x: Callable[[np.ndarray], np.ndarray]) -> int:
    return np.array(x > 0, dtype=np.int)
    
x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![step-function-graph](https://i.imgur.com/iZlkQDy.png)

0을 경계로 출력이 0에서 1로 변한다.

### 시그모이드 함수 구현하기

```
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
```

> `np.exp(-1)`은 `exp(-1)` 수식에 해당됨.

numpy의 브로드캐스팅 기능 때문에 numpy 배열도 위 함수에서 처리 가능하다.

```
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```


![sigmoid-graph](https://i.imgur.com/nmqfA2w.png)


### 비선형 함수

계단 함수와 sigmoid 함수는 여러가지 공통점과 차이점이 있다.

중요한 공통점은 둘 다 비선형 함수라는 것.

> 선형 함수: 출력 값이 입력의 상수배만큼 변하는 함수. `f(x) = ax + b`

신경망에서는 활성화 함수로 비선형 함수를 사용해야 함. 선형 함수를 사용하면 신경망의 층을 깊게 하는게 의미가 없어지기 때문.

### ReLU 함수

시그모이드 함수는 신경망 분야에서 오래전부터 이용했으나 최근에는 Rectified Linear Unit (ReLU) 함수를 주로 이용한다.

ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고 0 이하이면 0을 출력하는 함수.

```
h(x) = x if x > 0
h(x) = 0 if x <= 0
```

아주 간단한 함수이기 때문에 쉽게 구현 가능하다.

```
def relu(x):
    return np.maximum(0, x)
```

### 다차원 배열의 계산

행렬의 곱을 numpy에서 구현하기

```
>>> import numpy as np
>>> A = np.array([[1, 2], [3, 4]])
>>> B = np.array([[5, 6], [7, 8]])
>>> A.shape
(2, 2)
>>> B.shape
(2, 2)
>>> np.dot(A, B)
array([[19, 22],
       [43, 50]])
```

다차원 배열을 곱하려면 두 행렬의 대응하는 차원의 원소 수를 일치시켜야 됨.

### 3층 신경망 구현하기

```
import numpy as np

def sigmoid(x):
  return 1 / (1 + np.exp(-x))
  
  
# 첫번째 층
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

# (1x2) * (2x3) -> (1x3)
A1 = np.dot(X, W1) + B1
Z1 = sigmoid(A1)

# 두번째 층
W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

print(Z1.shape)
print(W2.shape)
print(B2.shape)

# (1x3) * (3x2) -> (1x2)
A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

# 출력층
def identity_function(x):
    return x
    
W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

# (1x2) * (2x2) -> (1x2)
A3 = np.dot(Z2, W3) + B3
Z3 = identity_function(A3)
```

### 구현 정리

위의 구현을 `init_network`와 `forward` 함수를 통해 구현. 

> `forward`는 신호가 순방향 (입력에서 출력으로)으로 전달됨을 나타내기 위해서.

```
def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    
    return network
    
def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)
    
    return y
  

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)
```

### 출력층 설계하기

신경망은 분류, 회귀 모두 사용됨. 다만 출력층에서 사용하는 활성화 함수가 달라짐.

- 회귀: 항등 함수
- 분류: 소프트맥스 함수

항등함수(identity function): 입력을 그대로 출력

소프트맥스 함수(softmax functino)

![softmax-function](https://lh3.googleusercontent.com/proxy/ujhMG9G1Clo2O2QwNJqBgIQerzQafxmSlHqLbJPQNmtGbdZBTGWEeewMFdjb2dw3qXBWRFm-Ii30mQS4n--VnM2fJnFHcaomqhEFtudT1TM)
- `exp(x)`는 지수 함수
- `j`은 출력층의 뉴런 수
- 소프트맥스 함수의 분자는 입력 신호의 지수함수
- 소프트맥수 함수의 분모는 모든 입력 신호의 지수의 합

```
>>> a = np.array([0.3, 2.9, 4.0])
>>>
>>> exp_a = np.exp(a)
>>> exp_a
array([ 1.34985881, 18.17414537, 54.59815003])
>>> sum_exp_a = np.sum(exp_a)
>>>
>>> sum_exp_a
74.1221542101633
>>>
>>> y = exp_a / sum_exp_a
>>> y
array([0.01821127, 0.24519181, 0.73659691])
```

위 인터프리터로 구현한 softmax를 함수로 구현하면 다음과 같음

```
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

### softmax 함수 주의점

softmax 함수를 컴퓨터로 구현할 때에는 오버플로 문제에 주의해야 함

> overflow: 컴퓨터는 수를 크기가 유한한 데이터 (ex. 4byte, 8byte)로 다루기 때문에 너무 큰 값을 계산하면 값을 표현하지 못할 수 있다.

이를 해결하기 위해 softmax 함수를 개선한 수식을 사용.

![softmax-overflow](https://www.burx.vip/wp-content/uploads/2019/07/1-57.png)

- `C`라는 임의의 정수를 분자/분모에 곱
- 지수함수 `exp()` 안으로 옮겨 `logC`로 만듦
- `logC`를 `C'` 기호로 변환 (쉬운 표현을 위해서)

> 위 식이 말하는 것은 softmax 지수 함수를 계산할 떄 어떤 정수를 더하거나 빼도 결과는 바뀌지 않는다는 것

그래서 overflow를 막기 위해 일반적으로 입력 신호 중 최대값을 이용. 아래는 예시 코드

```
>>> a = np.array([1010, 1000, 990])
>>> np.exp(a) / np.sum(np.exp(a))  # normal softmax function
__main__:1: RuntimeWarning: overflow encountered in exp
__main__:1: RuntimeWarning: invalid value encountered in true_divide
array([nan, nan, nan])
>>>
>>> c = np.max(a)
>>> a - c
array([  0, -10, -20])
>>> np.exp(a - c) / np.sum(np.exp(a - c))
array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])
```

그냥 계산하면 overflow가 발생하여 `nan`으로 출력되는 것을 볼 수 있음.

이를 적용한 softmax 함수

```
def softmax(a):
    max_input = np.max(a)
    exp_a = np.exp(a - max_input)  # avoid overflow
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

### softmax 함수의 특징


```
>>> a  = np.array([0.3, 2.9, 4.0])
>>> y = softmax(a)
>>> y
array([0.01821127, 0.24519181, 0.73659691])
>>> np.sum(y)
1.0
```

위 예시에서 볼 수 있듯이 softmax 함수의 출력은 0에서 1 사이이고 총 합은 1이다.

- 이때문에 softmax 함수의 출력을 확률로 해석이 가능하다.

softmax 함수를 적용해도 원소의 대소 관계는 변하지 않음.

- `exp(x)`가 단조 증가 함수이기 때문 (정의역 원소 a, b가 a < b 일때 f(a) <= f(b)가 성립하는 함수)
- 따라서 춢력이 가장 큰 뉴련의 위치가 달라지지 않음
- 신경망으로 분류할 떄에는 출력층의 softmax 함수를 생략해도 됨
- 현업에서는 자원 낭비를 줄이고자 생략하는게 일반적이라고 함.

### 손글씨 숫자 인식

MNIST 손글씨 숫자 데이터셋을 사용하여 실제 학습된 매개변수를 사용하여 추론 과정을 구현.

`(28 x 28)`의 회색조 (channel = 1) 이미지들 사용.

전처리 과정으로 정규화 (데이터를 특정 범위로 변환하는 처리)를 거쳐 픽셀 값을 0.0 ~ 1.0 사이의 값으로 만들어줌.

예시에서는 (1 x 28 x 28)의 3차원 배열을 `784`의 1차원 배열로 변환해서 진행


## 04. 신경망 학습

### 데이터 주도 학습

- 알고리즘을 밑바닥부터 사람이 설계하는 대신 특징을 추출하고 그 패턴을 학습하는 방법
- 신경망(딥러닝): 위와는 다르게 end-to-end machine learning.

훈련 데이터와 테스트 데이터를 나누는 이유?
- 범용 능력을 제대로 평가하기 위해
- 아직 보지 못한 데이터(훈련데이터)로도 문제를 올바르게 풀어내게
- 오버피팅: 한 데이터셋에만 지나치게 최적화된 상태


### 손실 함수

신경망 학습에서는 현재의 상태를 하나의 지표로 표현. 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색

이 지표를 손실 함수 (loss function)라고 함.

