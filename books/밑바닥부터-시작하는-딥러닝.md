# 밑바닥부터 시작하는 딥러닝



## 01. 헬로 파이썬

### numpy

```
# broadcasting
A = np.array([[1, 2], [3, 4]])
B = np.array([10, 20])

A * B

[ins] In [9]: X
Out[9]: array([51, 55, 14, 19,  0,  4])

[ins] In [10]: X > 15
Out[10]: array([ True,  True, False,  True, False, False])

[ins] In [11]: X[X>15]
Out[11]: array([51, 55, 19])
```


## 02. 퍼셉트론

### 퍼셉트론이란?

perceptron 알고리즘: 다수의 신호를 입력으로 받아 하나의 신호를 출력

- 입력 신호가 뉴런(노드)에 보내질 때 각각 가중치가 곱해짐
- 이 총 합이 정해진 한계(임계값)를 넘을때에만 1을 출력
- 아닐 경우 0을 출력

```
y = 0 if (w1 * x1 + w2 * x2) >= point
y = 1 if (w1 * x1 + w2 * x2) < point
```

이를 만족하는 w1, w2, theta (임계값) 조합은 여러개가 있을 수 있음



```
(w1, w2, theta)
# AND의 경우
(1, 1, 1)
(0.5, 0.5, 0.7)
...

# OR의 경우
(1, 1, 0)
(0.5, 0.5, 1)
...
```


### 파이썬으로 퍼셉트론 구현

```
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```

### 가중치와 편향 도입

위의 식을 다른 방식으로 표현하면 아래와 같다. 여기에서 `b`를 편향 (bias)이라고 함

```
y = 0 if (b + w1*x1 + w2*x2) <= 0
y = 1 if (b + w1*x1 + w2*x2) > 0
```

이를 numpy array로 표현해보면 다음과 같음

```
def AND(x1, x2):
    w1, w2 = 0.5, 0.5
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
        
def NAND(x1, x2):
    w1, w2 = 0.5, 0.5
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
        
def OR(x1, x2):
    w1, w2 = 0.5, 0.5
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

기존에 theta가 편향 b로 치환됨. (theta -> -b)

위 함수에서 가중치 (w, b)만 변경하면 AND 대신 OR, NAND도 구현 가능하다.

### 퍼셉트론의 한계

위에서 AND, OR, NAND를 구현해 보았는데 이번에는 XOR 게이트를 구현해보자.

> XOR 게이트: 배타적 논리합. 한 쪽이 1일때에만 1을 출력.

> 파이썬에서는 `^` operator가 XOR을 의미한다.

XOR는 퍼셉트론으로 표현하지 못하는 것은 그래프를 그리면 쉽게 알 수 있다.

(0, 0), (1, 1), (0, 1), (1, 0) 네 점 중에서 0, 1을 출력하는 점들을 직선으로 나눌 수 있다면 퍼셉트론으로 구현이 가능하다.

반면 XOR의 진리표는 다음과 같다.

```
| x1 | x2 | y |
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |
```

이를 x1, x2 축을 가진 그래프에 찍어보면 직선으로 0, 1을 나누는 영역을 그릴 수 없다.

### 다중 퍼셉트론

퍼셉트론을 층을 쌓아 multi-layer perceptron을 만들 수 있다.

XOR를 다중 퍼셉트론으로 구현해보자.

일단 제일 쉬운 방법은 OR, AND, NAND를 조합하여 만드는 방법.

```
# AND
| x1 | x2 | y |
| 0  | 0  | 0 |
| 1  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 1  | 1 |

# OR
| x1 | x2 | y |
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 1 |

# NAND
| x1 | x2 | y |
| 0  | 0  | 1 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |
```

OR, NAND의 게이트를 적용시키고 그 결과를 다시 AND에 입력하면 XOR를 구현할 수 있다.

![img](https://i.imgur.com/6hoX7A6.png)

이를 파이썬으로 구현해보면 다음과 같다.

```
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```


이 퍼셉트론으로 간단한 함수부터 컵퓨터 같은 복잡한 함수도 표현 가능하다.

## 02. 신경망

퍼셉트론으로 복잡한 함수도 표현이 가능하지만, 가중치는 여전히 사람이 수동으로 설정해야 함.

신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습 가능.

### 퍼셉트론 복습

기존 퍼셉트론의 함수를 `h(x)`를 사용하여 감싸보자

```
# 기존 함수
y = 0 if (b + w1*x1 + w2*x2) <= 0
y = 1 if (b + w1*x1 + w2*x2) > 0

# 활성화 함수
def h(x):
    if x <= 0:
        return 0
    elif x > 0:
        return 1
        
y = h(b + w1*x1 + w2*x2)
```

위와 같이 입력 신호의 총 합을 출력 신호로 변환하는 함수를 활성화 함수 (activation function)이라고 함.

### 활성화 함수

위의 활성화 함수가 퍼셉트론에서 신경망으로 가기 위한 길잡이.

퍼셉트론에서는 활성화 함수로 계단 함수를 이용.

> 계단 함수(step function): 임계값을 경계로 출력이 바뀌는 함수

계단 함수 이외의 함수를 사용한다면? 실제로 신경망에서는 다른 함수를 활성화 함수로 사용함.

다음부터 신경망에서 사용하는 활성화 함수 소개.

### 시그모이드 함수

![img](https://i.imgur.com/ZhXXone.png)

이를 함수로 표현해보면

```
h(x) = 1/(1 + exp(-x)
```

> `e`는 자연상수로 `2.7182...`의 값을 갖는 실수

신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환.

퍼셉트론과 신경망의 주된 차이는 활성화 함수뿐 뉴런이 여러 층으로 이어지는 구조와 신호를 전달하는 방법은 기본적으로 퍼셉트론과 같다.

### 계단 함수 구현하기

함수 이해를 위해 직접 파이썬으로 먼저 계단 함수를 구현

```
def step_function(x1 -> int, x2 -> int) -> int:
    if x > 0:
        return 1
    else:
        return 0
```

가장 간단한 구현이지만 x로 실수만 받을 수 있다.

넘파이 배열을 인수로 넣기 위해서 다음과 같이 변경.

```
from typing import Callable

def step_function(x: Callable[[np.ndarray], np.ndarray]) -> int:
    y = x > 0
    return y.astype(np.int)
```

numpy 배열에 부등호 연산을 수행하면 bool 배열을 반환하고, `astype`을 사용하여 int형으로 변환해준다.

위 함수의 그래프를 그려보자.

```
from typing import Callable
import numpy as np
import matplotlib.pylab as plt

def step_function(x: Callable[[np.ndarray], np.ndarray]) -> int:
    return np.array(x > 0, dtype=np.int)
    
x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![step-function-graph](https://i.imgur.com/iZlkQDy.png)

0을 경계로 출력이 0에서 1로 변한다.

### 시그모이드 함수 구현하기

```
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
```

> `np.exp(-1)`은 `exp(-1)` 수식에 해당됨.

numpy의 브로드캐스팅 기능 때문에 numpy 배열도 위 함수에서 처리 가능하다.

```
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```


![sigmoid-graph](https://i.imgur.com/nmqfA2w.png)


### 비선형 함수

계단 함수와 sigmoid 함수는 여러가지 공통점과 차이점이 있다.

중요한 공통점은 둘 다 비선형 함수라는 것.

> 선형 함수: 출력 값이 입력의 상수배만큼 변하는 함수. `f(x) = ax + b`

신경망에서는 활성화 함수로 비선형 함수를 사용해야 함. 선형 함수를 사용하면 신경망의 층을 깊게 하는게 의미가 없어지기 때문.

### ReLU 함수

시그모이드 함수는 신경망 분야에서 오래전부터 이용했으나 최근에는 Rectified Linear Unit (ReLU) 함수를 주로 이용한다.

ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고 0 이하이면 0을 출력하는 함수.

```
h(x) = x if x > 0
h(x) = 0 if x <= 0
```

아주 간단한 함수이기 때문에 쉽게 구현 가능하다.

```
def relu(x):
    return np.maximum(0, x)
```

### 다차원 배열의 계산




